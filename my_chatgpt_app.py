import streamlit as st
from langchain.chat_models import ChatOpenAI
import openai
from openai import OpenAI
from langchain.schema import (SystemMessage, HumanMessage, AIMessage)
import io
from streamlit.components.v1 import html
import base64
from audio_recorder_streamlit import audio_recorder
from tempfile import NamedTemporaryFile
import os

# Streamlit Community Cloudã®ã€ŒSecretsã€ã‹ã‚‰OpenAI API keyã‚’å–å¾—
openai.api_key = st.secrets.OpenAIAPI['openai_api_key']

client = OpenAI(
    # defaults to os.environ.get("OPENAI_API_KEY")
    api_key=openai.api_key,
)
#client = openai.api_key


def transcribe_audio_to_text(audio_bytes):
    # Create a temporary file and write the audio bytes to it
    with NamedTemporaryFile(mode='w+b', suffix=".wav") as temp_file:
        temp_file.write(audio_bytes)
        temp_file.seek(0)  # Rewind the file to the beginning

        response = client.audio.transcriptions.create(
            model="whisper-1",
            file=temp_file.file  # Pass the file descriptor directly
        )

    # Assuming `response` has an attribute `text` with the transcribed content
    transcription_text = response.text if hasattr(response, 'text') else "No transcription attribute found."

    return transcription_text

def play_audio(byte_stream):
    # Base64ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰ã•ã‚ŒãŸéŸ³å£°ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—
    base64_audio = base64.b64encode(byte_stream.read()).decode()

    # ã‚«ã‚¹ã‚¿ãƒ ã®HTMLã¨JavaScriptã‚’ä½¿ã£ã¦éŸ³å£°ã‚’è‡ªå‹•å†ç”Ÿ
    audio_html = f"""
    <audio autoplay>
        <source src="data:audio/ogg;base64,{base64_audio}" type="audio/ogg">
        Your browser does not support the audio element.
    </audio>
    """

    # Streamlitã«ã‚«ã‚¹ã‚¿ãƒ ã®HTMLã‚’åŸ‹ã‚è¾¼ã‚€
    html(audio_html, height=0)  # height=0ã§ã‚¦ã‚£ã‚¸ã‚§ãƒƒãƒˆã®è¡¨ç¤ºã‚’éš ã™


def main():
    llm = ChatOpenAI(model="gpt-4-1106-preview", temperature=0, openai_api_key=openai.api_key)
    #llm = ChatOpenAI(model="gpt-4-1106-preview",temperature=0)

    st.set_page_config(
        page_title="ã›ã„ã¶ã¤ã¯ã‹ã›ã®ChatGPT",
        page_icon="ğŸ¤—"
    )
    st.header("Dr. Biology's ChatGPT ğŸ¤—")

    st.image("DALLÂ·E 2023-11-19 19.04.23 - An anime-style, chibi (two-head-tall) female biologist character .png")

    # ãƒãƒ£ãƒƒãƒˆå±¥æ­´ã®åˆæœŸåŒ–
    if "messages" not in st.session_state:
        st.session_state.messages = [
            SystemMessage(content="ç”Ÿç‰©ã®ã“ã¨ãªã‚‰ãªã‚“ã§ã‚‚çŸ¥ã£ã¦ã„ã‚‹å°å­¦ç”Ÿã®å¥³ã®å­ç”Ÿç‰©åšå£«ã¨ã—ã¦æŒ¯ã‚‹èˆã„ã¾ã™ã€‚ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•ã«å¯¾ã—ã¦ã€ã¿ã‚“ãªãŒé©šãã‚ˆã†ãªæ–°ã—ã„å‹•ç‰©ã®ãƒˆãƒªãƒ“ã‚¢ã‚’æ•™ãˆã¦ãã ã•ã„ã€‚ãƒˆãƒªãƒ“ã‚¢ã®å†…å®¹ã¯150æ–‡å­—ä»¥å†…ã§ä¼ãˆã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚å£èªçš„ã«è©±ã—ã¦ã€å„ªã—ã„å‹é”ã®ã‚ˆã†ãªè©±ã—æ–¹ã‚’ã—ã¦ãã ã•ã„ã€‚")
            #SystemMessage(content="You are a helpful assistant.")
        ]

    # Record audio using Streamlit widget
    audio_bytes = audio_recorder(pause_threshold=1.0)

    # Convert audio to text using OpenAI Whisper API
    if audio_bytes:
        transcript = transcribe_audio_to_text(audio_bytes)
        #st.write("Transcribed Text:", transcript)

        # ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®å…¥åŠ›ã‚’ç›£è¦–
        user_input = st.text_input("ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’å…¥åŠ›ã—ã¦ã­ï¼", value=transcript)
        if user_input:
        #if user_input := st.write("Transcribed Text:", transcript):
            st.session_state.messages.append(HumanMessage(content=user_input))
            with st.spinner("ChatGPT is typing ..."):
                response = llm(st.session_state.messages)
            st.session_state.messages.append(AIMessage(content=response.content))
            # OpenAIã®Text-to-Speech APIã‚’ä½¿ç”¨ã—ã¦å¿œç­”ã‚’éŸ³å£°ã«å¤‰æ›openai.audio.speech.create
            try:
                audio_response = openai.audio.speech.create(
                    model="tts-1",
                    voice="nova",
                    input=response.content,
                )

                # å–å¾—ã—ãŸéŸ³å£°ãƒ‡ãƒ¼ã‚¿ã‚’ãƒã‚¤ãƒˆã‚¹ãƒˆãƒªãƒ¼ãƒ ã«å¤‰æ›ã—ã€å†ç”Ÿ
                byte_stream = io.BytesIO(audio_response.content)
                # è‡ªå‹•å†ç”Ÿã™ã‚‹ãŸã‚ã®é–¢æ•°ã‚’å‘¼ã³å‡ºã™
                play_audio(byte_stream)
            except Exception as e:
                st.error(f"éŸ³å£°ç”Ÿæˆã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")

        # ãƒãƒ£ãƒƒãƒˆå±¥æ­´ã®è¡¨ç¤º
        messages = st.session_state.get('messages', [])
        for message in messages:
            if isinstance(message, AIMessage):
                with st.chat_message('assistant'):
                    st.markdown(message.content)
            elif isinstance(message, HumanMessage):
                with st.chat_message('user'):
                    st.markdown(message.content)
            # SystemMessageã®å‡¦ç†ã‚’å‰Šé™¤ã¾ãŸã¯ã‚³ãƒ¡ãƒ³ãƒˆã‚¢ã‚¦ãƒˆ
            #else:  # isinstance(message, SystemMessage):
                #st.write(f"System message: {message.content}")

if __name__ == '__main__':
    main()